---
title: "Homework 6"
author: "Brad May"
date: "February 20, 2018"
output:
  word_document: default
  html_document: default
---

####Question 9.1

**Using the same crime data set as in Question 8.2, apply Principal Component Analysis and then create a regression model using the first few principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Question 8.2. You can use the R function prcomp for PCA. Note that to first scale the data, you can include scale. = TRUE to scale as part of the PCA function. Don't forget that, to make a prediction for the new city, you'll need to unscale the coefficients (i.e., do the scaling calculation in reverse!)**

Let's start as we usually do by clearing our environment, loading the "stats" library which contains our functions of interest, setting the seed, and loading the data.

```{r}
# Principal Component Analysis (PCA) with crime statistics problem

# Clear environment
rm(list = ls())

# Load the "stats" library and set seed
library(stats)
set.seed(1)

# Load the dataset and convert it into a vector
data = read.table("uscrime.txt", header = TRUE, sep = "")
head(data)
```

We can see from the headers and initial data displayed that it appears our dataset loaded properly.  Now, we will set up our model by running the "prcomp" function and looking at some of the output generated by it including a plot of that output.

```{r}
# Run the "prcomp" function to obtain our PCA calculations of interest, look
# at some of the output data generated by it, and plot results
pca = prcomp(data[1:15], scale. = TRUE)
summary(pca)
plot(pca)
```

The model summary allows us to see the standard deviation, proportion of variance, and cumulative proportion of variance for each of the 15 principal components.  Focusing in on the cumulative proportion of variance allows us to see that about 80% of the variance is realized in the first four components while 90% is realized in the first six components.  The plot appears to confirm this.  We can view this visually via a "screeplot" and two other plots, one showing the variance proportion for each principal component added and the other showing the cumulative variance proportion over the full range of principal components added.

```{r}
# Generate a "screeplot" and some other plots to help determine how many
# principal components we will use in our follow-on analysis
screeplot(pca, type = "lines", col = "blue")
# Variance proportion & cumulative variance proportion plots
variance = pca$sdev ^ 2
prop_variance = variance / sum(variance)
plot(prop_variance, xlab = "Principal Component", ylab = "Variance Proportion", 
     ylim = c(0, 1), type = "b")
plot(cumsum(prop_variance), xlab = "Principal Component", 
        ylab = "Cumulative Variance Proportion", 
        ylim = c(0, 1), type = "b")
```

From these plots, we notice that, beyond six components, we don't get much of a significant increase in the incremental proportion of variance.  The plot seems to flatten out rather quickly.  Thus, we will use six principal components in all our follow-on analysis.  These six should contain sufficient information to adequately solve our problem.

We first extract the transformed x-values from the first six principal components.  We then append the "Crime" response onto this.  Lastly, we convert this to a data frame for use in R's "lm" regression function and inspect the data.

```{r}
# Use six components and set up the regression
t = pca$x[, 1:6] # First six components over all observations
t = cbind(t, Crime = data[, 16]) # Append the "Crime" data to the matrix
t_df = data.frame(t) # Convert to data frame
head(t_df) # View first few observations of data
```

Everything appears to be in order so we will run the "lm" regression function to obtain our regressor.  Then, we will view the regression summary data.

```{r}
# Perform the regression and summarize
regressor = lm(formula = Crime ~ ., data = t_df)
summary(regressor)
```

Now that we have "transformed" regression coefficients (b0...bn), let's transform them back to coefficients that can be used in predictions (a0...an).

```{r}
# Obtain the b0 and bn coefficients from the regressor summary
b_zero = regressor$coefficients[1]
b_n = regressor$coefficients[2:7]

# Convert bn coefficients to "an" coefficients
a_n = pca$rotation[, 1:6] %*% b_n
# Unscale back to original values
orig_a_n = a_n / sapply(data[, 1:15], sd)
orig_b_zero = b_zero - sum(a_n * sapply(data[, 1:15], mean) / 
        sapply(data[, 1:15], sd))
```

And, now that we have coefficients that can be used in predictions, let's try to predict the result given the predictor values provided in Question 8.2.

```{r}
# Load dataframe that contains the values for which we want to generate prediction
pred_df = data.frame(M = 14.0, So = 0, Ed = 10.0, Po1 = 12.0, Po2 = 15.5, 
                     LF = 0.640, M.F = 94.0, Pop = 150, NW = 1.1, U1 = 0.120, 
                     U2 = 3.6, Wealth = 3200, Ineq = 20.1, Prob = 0.04, Time = 39.0)
head(pred_df)

# Generate prediction for given data
prediction = orig_b_zero
for(i in 1:length(orig_a_n)) {
        prediction = prediction + orig_a_n[i] * pred_df[i]
}
prediction
```

Our prediction is about 1248 which compares favorably to the 1304 prediction obtained while answering Question 8.2.

Now, let's calculate R-squared and adjusted R-squared for our new model.

```{r}
# Calculate R2 and adjusted R2
estimates = as.matrix(data[, 1:15]) %*% orig_a_n + orig_b_zero
SSE = sum((estimates - data[, 16]) ^ 2)
SSTotal = sum((data[, 16] - mean(data[, 16])) ^ 2)
R2 = 1 - SSE / SSTotal
R2
Adj_R2 = R2 - (1 - R2) ^ 6 / (nrow(data) - 6 -1)
Adj_R2
```

Our R-squared is now 65.9%.  In Question 8.2, we obtained an R-squared of 76.6% for the model using the best predictors.  However, one thing to note is that the adjusted R-squared in our current PCA model drops to about 65.8%, only about a one-tenth of one percent difference from R-squared.  Our model from Question 8.2 had adjusted R-squared drop about three percentage points from the R-squared value.  Thus, our PCA model appears to possess a lot less variance than the model developed previously.

One last thing we will do is to take a look at what happens to R-squared over the full range of components.

```{r}
# How does R2 & adjusted R2 change as we add principal components?
for (i in 1:15) {
        t = round(pca$x[, 1:i], 4)
        t = cbind(t, Crime = data[, 16])
        t_df = data.frame(t)
        regressor = lm(formula = Crime ~ ., data = t_df)
        R2[i] = 1 - sum(regressor$residuals ^ 2) / 
                sum((data$Crime- mean(data$Crime)) ^ 2)
}
plot(R2, xlab = "Principal Component", ylab = "R-Squared", ylim = c(0, 1), 
     type = "b")
```

We see from this graph that our choice of six components was a pretty good one.  R-squared does not improve much beyond six components.  In fact, we probably could have gone with five components and obtained results that were about as good as what we got using six.

---
title: "ISYE 6501 Homework Week 3"
author: "Brad May"
date: "January 26, 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

####Question 5.1

**Using crime data from http://www.statsci.org/data/general/uscrime.txt (description at http://www.statsci.org/data/general/uscrime.html), test to see whether there is an outlier in the last column (number of crimes per 100,000 people). Is the lowest-crime city an outlier? Is the highest-crime city an outlier? Are there others? Use the grubbs.test function in the outliers package in R.**

First, let's clear the environment and then load the "outliers" library where the "grubbs.test" function resides.  Then, we'll load the data set "uscrime.txt" from the working directory.

```{r}
# Crime data outlier problem

# Clear environment
rm(list = ls())

# Load the "outliers" library
library(outliers)

# Load the dataset
data = read.table("uscrime.txt", header = TRUE, sep = "")
```

Once we have the data set loaded, let's plot the variable of interest ("Crimes").
The plot shows values for the "Crimes" variable on the y-axis with the index number for each of those "Crimes" variable values on the x-axis.

```{r}
# Plot the "Crime" values by their respective observation number
plot(x = seq(1, nrow(data), 1), y = data$Crime, xlab = "Observation Number", 
     ylab = "Crime")
```

From this plot, we notice two potential outliers up around the 2000 crimes per 100k people value with possibly three more around the 1500 crimes per 100k people value.  We'll see which, if any, of these turn out to be actual outliers when we run "grubbs.test".

Next, let's run a "qqnorm" plot on the data.  The "grubbs.test" function requires a dataset with a normal distribution.  So, by inspecting the "qqnorm" plot, we can make an assessment as to how "normal" the data is.

```{r}
# Run "qqnorm" plot on the dataset to inspect for "normality" of its distribution.
qqnorm(scale(data$Crime))
```

The sample data has been scaled for easy comparison to the theoretical values.  From this normal Q-Q plot, we see a few datapoints, all in the upper right of the chart, that do not look to be part of a normal distribution.  These could be outliers, though.  If they were removed as outliers, the remaining datapoints look to be fairly normally distributed.  So, we will continue with the Grubbs test and see if these points are identified as outliers by the test.

Next, we'll run the Grubbs test on the data.  We are running type = 10 with a one-sided test and opposite = FALSE.  This will cause the function to find either the highest or lowest value from the data that is most likely to be an outlier.  Then, it will provide some statistics back on that potential outlier so that we can determine whether or not it is actually an outlier based on criteria we will set.

```{r}
# Run "grubbs.test" to determine if any outliers exist in the "Crime" column
test = data$Crime
outliers = grubbs.test(test, type = 10, opposite = FALSE, two.sided = FALSE)
outliers
```

This then shows that the highest value of 1993 crimes per 100k people is potentially an outlier.  How do we determine whether or not it is for sure?  Look at the p-value which is equal to 0.079.  The Grubbs test goes in with a null hypothesis that no outliers exist in the data.  There is also an alternative hypothesis that there is exactly one outlier in the data.  In this case, that value is 1993 crimes per 100k people.  The p-value provides a measure of how likely it is that we can reject the null hypothesis.  The larger the p-value gets, the more likely it becomes that we cannot reject the null hypothesis and thus assume that there are no outliers in the data.  Thus, the key to determining whether or not 1993 crimes per 100k people is an outlier lies in setting a threshold for the p-value and seeing where the actual p-value arrives relative to that threshold.

I am choosing a p-value threshold of 0.1.  So long as the p-value stays below 0.1, we can reject the null hypothesis and assume that the value returned by the Grubbs test is, in fact, an outlier.  Thus, the 1993 crimes per 100k people returned by the Grubbs test initially is an outlier since its p-value = 0.079 which is less than 0.1.

Next, we'll run some code to generate all the outliers contained in the data set for the "Crimes" variable.  The code runs through the following sequence:

1. Takes the results from running "grubbs.test" the first time on the complete "Crimes" variable data set.  This is described above.
2. Recall from the previous disccusion that the Grubbs test determined the most likely outlier at either the high or low end of the dataset distribution.  In our example, it chose 1993 crimes per 100k people at the high end of the "Crimes" data set distribution.
3. The "grubbs.test" function also returned several objects including the value of the outlier and its associated p-value.  These are assigned to variables.
4. So long as the p-value is less than 0.1, the outlier value returned by the Grubbs test is captured in a vector called "outlier_list".
5. The outlier value calculated is then removed from the data set and "grubbs.test" re-run on the modified "Crimes" variable array that now has the outlier removed.
6. This process is repeated as long as the p-value stays below 0.1 and all pertinent outlier values are captured in "outlier_list".
7. Once the process completes, the full list of outliers is displayed.

```{r}
# Determine all outliers that exist in the dataset
# Pull out the outlier and p-value and assign them to variables
result = as.numeric(strsplit(outliers$alternative," ")[[1]][3])
pv = outliers$p.value
# Initialize "outlier_list" vector that will store all outliers identified
outlier_list = NULL
while (pv < .1) { # Loop runs so long as p-value is below 0.1
        outlier_list = c(outlier_list, result) # Add outlier to "outlier_list"
        test = test[!test %in% result] # Remove outlier from dataset
        # Re-run Grubbs test on dataset with outlier removed
        outliers = grubbs.test(test, type = 10, opposite = FALSE, 
                               two.sided = FALSE)
        # Assign new outlier and corresponding G-statistic to variables
        result = as.numeric(strsplit(outliers$alternative," ")[[1]][3])
        pv = outliers$p.value
}
# Print out "outlier_list" vector containing all identified outliers
outlier_list
outliers
```

From this we see that two outliers were identified, all at the high end of the dataset distribution.  These correspond to the two datapoints in the upper right-most corner of the Normal Q-Q plot.  They also correspond to the two points on the initial plot of the data that showed up around the 2000 crimes per 100k people value.  Thus, the two highest crime cities appear to be outliers.

I also captured the results of "grubbs.test" for the next datapoint identified as a potential outlier after the two in "outlier_list".  We do this to check the statistics for that point and ensure that it makes sense that this datapoint was not determined to be an outlier.  The datapoint was 1674 crimes per 100k people and, given that it has a p-value of 0.178 which is greater than 0.1, it does make sense that we are not calling this point an outlier.

Lastly, I took a look at the possibility of the lowest crime city being an outlier.  To do this, I ran "grubbs.test" with "opposite" set to TRUE.  That will force the test to give us the lowest value as an outlier along with its associated statistics.

```{r}
# Run "grubbs.test" to determine if any outliers exist in the "Crime" column at
# the low end of the distribution
test = data$Crime
outliers = grubbs.test(test, type = 10, opposite = TRUE, two.sided = FALSE)
outliers
```

This returns the lowest value of 342 crimes per 100k people as a potential outlier.  However, given that its p-value is 1, much greater than 0.1, we conclude that none of the lowest crime cities is an outlier.


####Question 6.1

**Describe a situation or problem from your job, everyday life, current events, etc., for which a Change Detection model would be appropriate. Applying the CUSUM technique, how would you choose the critical value and the threshold?**

Years ago, I used to work in a manufacturing plant for Frito-Lay.  In a production environment, up-time is everything.  If you go down unexpectedly and are down for a while, a whole cascading series of events must take place whereby the master scheduling staff must figure out where extra product must now come from to make up the shortfall and meet expected demand.  Most often, unexpected downtime resulted from equipment failure.  So, it would make sense that a Change Detection model might be appropriate for this application.  By measuring changes in certain parameters associated with running equipment, you might be able to detect impending equipment failure and head it off before it results in an unexpected production line shutdown.

Since most of the equipment that could fail was either pumps or motors, I would say the most pertinent parameters to measure would be things like bearing temperatures, flow velocity, and electrical readings related to the equipment power supplies.  All of the equipment would come with a series of manufacturer specifications (specs).  These could tell you normal operating ranges, dangerous operating conditions, maximum sustainable operating ranges, etc.  This would be a good starting point in determining critical values and thresholds.  Discussions with manufacturer's representatives could provide another good source of useful input.  Finally, any historical data that could be gathered or had already been collected would be extremely valuable.  Particularly if you had the data cross-referenced against past equipment failures.  These data could also inform the decision regarding what to set as critical values and thresholds for various pieces of operating equipment.


####Question 6.2

**1. Using July through October daily-high-temperature data for Atlanta for 1996 through 2015, use a CUSUM approach to identify when unofficial summer ends (i.e., when the weather starts cooling off) each year. You can get the data that you need from the file temps.txt or online, for example at http://www.iweathernet.com/atlanta-weather-records or https://www.wunderground.com/history/airport/KFTY/2015/7/1/CustomHistory.html . You can use R if you'd like, but it's straightforward enough that an Excel spreadsheet can easily do the job too.**

I used Excel exclusively to solve this problem.  For each year, I calculated the mean and standard deviation for temperatures in July.  I used July figuring those temperatures would be most indicative of the actual summer descriptive statistics.  I then calculated the cumulative sum (CUSUM) by year.  The formula I used to do this was as follows:

***2. Use a CUSUM approach to make a judgment of whether Atlanta's summer climate has gotten warmer in that time (and if so, when).***

